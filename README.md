# Realtime Voice API Backend

This Node.js backend server acts as a secure and efficient proxy between a web-based frontend client and the OpenAI Realtime Voice API. It facilitates building real-time voice chat applications by handling WebSocket connections, API authentication, audio streaming, session management, and saving the transcripts generated by the AI along with date metadata.

## Overview

The primary function of this server is to:

1. **Establish WebSocket Connections**: Accept WebSocket connections from frontend clients.
2. **Authenticate with OpenAI**: Securely use your OpenAI API key to connect to their Realtime Voice API.
3. **Proxy Audio Streams**:
   - Receive raw audio chunks (PCM16) from the client.
   - Forward this audio to the OpenAI API.
   - Receive synthesized audio chunks (PCM16) from OpenAI.
   - Stream this audio back to the connected client.
4. **Stream Text Deltas**: Receive real-time transcription/response text deltas from OpenAI and forward them to the client.
5. **Save Transcript/Text Data**: Save the final transcriptions and AI-generated text responses to the server’s file system, along with timestamps. These saved text responses are stored with date metadata for archival or debugging purposes.
6. **Manage Sessions**: Handle the lifecycle of the OpenAI Realtime session, including configuration (voice, audio formats, VAD).
7. **Enforce Security**: Implement CORS policies to ensure only allowed frontend origins can connect.
8. **Provide Logging & Error Handling**: Log key events and manage potential errors gracefully.
9. **(Optional) Save Audio**: Persist the audio generated by OpenAI to the server's local filesystem for debugging or archival.

## Features

- **Real-time Bi-directional Audio Streaming**: Seamlessly pipes audio between the client and OpenAI.
- **Real-time Text Streaming**: Delivers OpenAI's text responses incrementally (`response.text.delta`).
- **WebSocket Communication**: Uses WebSockets for low-latency communication with both the client and OpenAI.
- **Secure API Key Handling**: Loads OpenAI API key from environment variables (`.env` file).
- **CORS Protection**: Restricts WebSocket connections to allowed origins.
- **Dynamic Session Configuration**: Configures OpenAI session parameters (voice, audio formats, VAD) upon connection.
- **Graceful Shutdown**: Handles `SIGTERM` signals to close connections cleanly.
- **Basic Logging**: Provides console output for connection events, errors, and key OpenAI messages.
- **Audio Output Saving**: Saves generated raw PCM audio files to `openai_audio_output/`.
- **Audio Output Saving**: Saves generated text files to `openai_text_output/`.
- **Transcript Saving with Date Data**: Saves the full transcript of each AI interaction, including the date and time of the conversation. These files are stored in a directory for easy access and debugging.

## Prerequisites

- **Node.js**: (v18.x or later recommended)
- **npm** or **yarn**
- **OpenAI Account**: You need an account with OpenAI.
- **OpenAI API Key**: An API key with access to the Realtime Voice API models.

## Setup & Installation

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/Shreyas-Dayal/voice-chat-backend
   cd voice-chat-backend
   ```

2. **Install Dependencies**:

   ```bash
   npm install
   # or
   yarn install
   ```

3. **Create Environment File**:
   Create a `.env` file in the root directory of the backend project.

4. **Add API Key**:
   Open the `.env` file and add your OpenAI API key:

   ```dotenv
   # .env
   OPENAI_API_KEY=sk-YourActualOpenAiApiKeyHere
   ```

5. **(Optional) Configure Port**:
   You can optionally specify the server port in the `.env` file (defaults to 8080):

   ```dotenv
   # .env
   PORT=8080
   OPENAI_API_KEY=sk-YourActualOpenAiApiKeyHere
   ```

6. **Configure Allowed Origins**:
   Edit `server.ts` and update the `allowedOrigins` array to include the URL(s) of your frontend application:

   ```typescript
   // src/server.ts (or server.js)
   const allowedOrigins: string[] = [
     "http://localhost:5173",
     "https://your-frontend-domain.com",
   ];
   ```

## Running the Server

1. **Start the Server**:

   ```bash
   npm start
   # or (if you have a dev script using nodemon, etc.) - In Current implementation I have used nodemon
   # npm run dev
   # or directly using node
   # node server.js
   ```

2. The server will start, log its configuration, and listen for connections on the specified port (default 8080). You should see output similar to:

   ```
   Backend Configuration loaded successfully.
    > OpenAI Model: gpt-4o-mini-realtime-preview-2024-12-17
    > OpenAI Connection URL: wss://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview-2024-12-17
    > Expected Input Format: {"codec":"pcm","sample_rate":16000,"encoding":"pcm_s16le"}
    > Requested Output Format: {"codec":"opus","container":"ogg","sample_rate":24000}
   [Server] CORS enabled for origins: http://localhost:5173
   [Server] HTTP server started on port 8080
   [Server] WebSocket server listening on ws://localhost:8080
   ```

## API Interaction (Client-Side Perspective)

A frontend client needs to interact with this backend as follows:

1. **Connect**: Establish a WebSocket connection to `ws://<your-backend-host>:<PORT>` (e.g., `ws://localhost:8080`).
2. **Send Audio**: Send raw audio data as **binary WebSocket messages**. The audio _must_ match the format expected by the backend and configured for OpenAI's input:
   - **Format**: Raw PCM
   - **Sample Rate**: 16000 Hz
   - **Bit Depth**: 16-bit Signed Integer
   - **Endianness**: Little-Endian
3. **Receive Messages**: Listen for messages from the backend WebSocket:
   - **Binary Messages**: These contain raw audio chunks (PCM16, 16kHz) from OpenAI's response. The client needs to buffer and play this audio.
   - **Text (JSON) Messages**: These contain events and text data. Parse the JSON string:
     - `{ type: 'event', name: 'AIConnected', sessionId: '...' }`: OpenAI connection established.
     - `{ type: 'event', name: 'AIResponseStart' }`: OpenAI has started generating a response.
     - `{ type: 'event', name: 'AIResponseEnd', finalText: '...' }`: OpenAI has finished the response. `finalText` contains the complete assistant message text.
     - `{ type: 'event', name: 'AISpeechDetected' }`: OpenAI detected the start of user speech (if using server VAD).
     - `{ type: 'event', name: 'AISpeechEnded' }`: OpenAI detected the end of user speech (if using server VAD).
     - `{ type: 'textDelta', text: '...' }`: An incremental part of the assistant's text response. Concatenate these deltas to display the response as it's generated.
     - _(Other potential events or error messages)_

## OpenAI Configuration Details (Hardcoded in Handler)

The backend configuration explicitly requests the following during the `session.update` event:

- **Input Audio Format**: `pcm16` (16kHz, Signed Little Endian)
- **Output Audio Format**: `pcm16` (16kHz)
- **Voice**: `shimmer`
- **Turn Detection**: `server_vad` (Server-Side Voice Activity Detection)
- **Instructions**: `"You are a helpful voice assistant. Respond ONLY in English."`

## Saving Transcripts with Date Metadata

The backend also saves the full transcript of each AI interaction to the server. These transcripts are stored with the following structure:

- **File Name**: The filename includes the date and time of the interaction in the format `{timestamp}_{responseIdToSave}_transcript.xt`.
- **File Content**: Each transcript file contains the full text response from OpenAI, including incremental text deltas and the final AI response.

These transcript files are stored in a directory on the server (`/openai_text_output`), which is automatically created.

## Project Structure

```
.
│   .gitignore                # Files ignored by git (e.g., .env, node_modules, audio outputs)
│   package-lock.json          # Lockfile for reproducible installs
│   package.json               # Project metadata and dependencies
│   README.md                  # Project documentation
│   tsconfig.json              # TypeScript configuration file
├───openai_audio_output        # Directory where generated audio is saved (created automatically)
├───openai_text_output         # Directory where generated AI text responses are saved (created automatically)
├───src
│       config.ts              # Loads .env, defines configuration constants (PORT, API Key, OpenAI base URL/model)
│       openaiHandler.ts       # Logic for handling OpenAI API interaction per client
│       server.ts              # Main server setup, WebSocket connection handling, CORS
└───transcripts               # Directory where the transcript files are saved (created automatically)
```

## Troubleshooting

- **Connection Refused**: Ensure the server is running and you are connecting to the correct host and port from your frontend. Check firewall rules.
- **CORS Errors**: Verify the `allowedOrigins` in `server.ts` exactly matches your frontend's origin (including protocol `http/https`, host, and port). Check the server console logs for CORS rejection messages.
- **401 Unauthorized (from OpenAI)**: Double-check that your `OPENAI_API_KEY` in the `.env` file is correct and valid. Ensure the `.env` file is in the project root where you run the `node` command.
- **Audio Issues**:
  - Verify the frontend is sending audio in the correct format (PCM16, 16kHz, signed, little-endian).
  - Check the server console logs for errors during audio processing or sending/receiving from OpenAI.
  - Examine the `.raw` files saved in `openai_audio_output/` using an audio editor that supports raw PCM import (like Audacity) to verify the received audio.
- **Transcript Issues**:
  - Ensure the transcript directory (`/transcripts/`) exists and is accessible by the backend process.
  - Verify that the transcript files are being saved in the correct format.

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs, feature requests, or improvements.

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.
