# Realtime Voice API Backend

<!-- [![Node.js CI - Placeholder](https://github.com/your-username/your-repo-name/actions/workflows/node.js.yml/badge.svg)](https://github.com/your-username/your-repo-name/actions/workflows/node.js.yml)  -->
<!-- Replace with your actual CI badge URL -->
<!-- [![License: MIT - Placeholder](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)  -->
<!-- Choose and update your license -->

This Node.js backend server acts as a secure and efficient proxy between a web-based frontend client and the OpenAI Realtime Voice API. It facilitates building real-time voice chat applications by handling WebSocket connections, API authentication, audio streaming, and session management.

## Overview

The primary function of this server is to:

1.  **Establish WebSocket Connections:** Accept WebSocket connections from frontend clients.
2.  **Authenticate with OpenAI:** Securely use your OpenAI API key to connect to their Realtime Voice API.
3.  **Proxy Audio Streams:**
    - Receive raw audio chunks (PCM16) from the client.
    - Forward this audio to the OpenAI API.
    - Receive synthesized audio chunks (PCM16) from OpenAI.
    - Stream this audio back to the connected client.
4.  **Stream Text Deltas:** Receive real-time transcription/response text deltas from OpenAI and forward them to the client.
5.  **Manage Sessions:** Handle the lifecycle of the OpenAI Realtime session, including configuration (voice, audio formats, VAD).
6.  **Enforce Security:** Implement CORS policies to ensure only allowed frontend origins can connect.
7.  **Provide Logging & Error Handling:** Log key events and manage potential errors gracefully.
8.  **(Optional) Save Audio:** Persist the audio generated by OpenAI to the server's local filesystem for debugging or archival.

## Features

- **Real-time Bi-directional Audio Streaming:** Seamlessly pipes audio between the client and OpenAI.
- **Real-time Text Streaming:** Delivers OpenAI's text responses incrementally (`response.text.delta`).
- **WebSocket Communication:** Uses WebSockets for low-latency communication with both the client and OpenAI.
- **Secure API Key Handling:** Loads OpenAI API key from environment variables (`.env` file).
- **CORS Protection:** Restricts WebSocket connections to allowed origins.
- **Dynamic Session Configuration:** Configures OpenAI session parameters (voice, audio formats, VAD) upon connection.
- **Graceful Shutdown:** Handles `SIGTERM` signals to close connections cleanly.
- **Basic Logging:** Provides console output for connection events, errors, and key OpenAI messages.
- **Audio Output Saving:** Saves generated raw PCM audio files to `openai_audio_output/`.

## Prerequisites

- **Node.js:** (e.g., v18.x or later recommended)
- **npm** or **yarn**
- **OpenAI Account:** You need an account with OpenAI.
- **OpenAI API Key:** An API key with access to the Realtime Voice API models.

## Setup & Installation

1.  **Clone the Repository:**

    ```bash
    git clone <your-repository-url>
    cd <your-repository-directory>
    ```

2.  **Install Dependencies:**

    ```bash
    npm install
    # or
    yarn install
    ```

3.  **Create Environment File:**
    Create a `.env` file in the root directory of the backend project.

4.  **Add API Key:**
    Open the `.env` file and add your OpenAI API key:

    ```dotenv
    # .env
    OPENAI_API_KEY=sk-YourActualOpenAiApiKeyHere
    ```

    **Important:** Never commit your `.env` file or your API key to version control. Add `.env` to your `.gitignore` file.

5.  **(Optional) Configure Port:**
    You can optionally specify the server port in the `.env` file (defaults to 8080):

    ```dotenv
    # .env
    PORT=8080
    OPENAI_API_KEY=sk-YourActualOpenAiApiKeyHere
    ```

6.  **Configure Allowed Origins:**
    Edit `server.js` and update the `allowedOrigins` array to include the URL(s) of your frontend application:
    ```javascript
    // src/server.ts (or server.js)
    const allowedOrigins: string[] = [
      "http://localhost:5173",
      "https://your-frontend-domain.com",
    ];
    ```

## Running the Server

1.  **Start the Server:**

    ```bash
    npm start
    # or (if you have a dev script using nodemon, etc.)
    # npm run dev
    # or directly using node
    # node server.js
    ```

2.  The server will start, log its configuration, and listen for connections on the specified port (default 8080). You should see output similar to:
    ```
    Backend Configuration loaded successfully.
     > OpenAI Model: gpt-4o-mini-realtime-preview-2024-12-17
     > OpenAI Connection URL: wss://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview-2024-12-17
     > Expected Input Format: {"codec":"pcm","sample_rate":16000,"encoding":"pcm_s16le"}
     > Requested Output Format: {"codec":"opus","container":"ogg","sample_rate":24000} // Note: Actual format requested is PCM16
    [Server] CORS enabled for origins: http://localhost:5173
    [Server] HTTP server started on port 8080
    [Server] WebSocket server listening on ws://localhost:8080
    ```

## API Interaction (Client-Side Perspective)

A frontend client needs to interact with this backend as follows:

1.  **Connect:** Establish a WebSocket connection to `ws://<your-backend-host>:<PORT>` (e.g., `ws://localhost:8080`).
2.  **Send Audio:** Send raw audio data as **binary WebSocket messages**. The audio _must_ match the format expected by the backend and configured for OpenAI's input:
    - **Format:** Raw PCM
    - **Sample Rate:** 16000 Hz
    - **Bit Depth:** 16-bit Signed Integer
    - **Endianness:** Little-Endian
3.  **Receive Messages:** Listen for messages from the backend WebSocket:
    - **Binary Messages:** These contain raw audio chunks (PCM16, 16kHz) from OpenAI's response. The client needs to buffer and play this audio.
    - **Text (JSON) Messages:** These contain events and text data. Parse the JSON string:
      - `{ type: 'event', name: 'AIConnected', sessionId: '...' }`: OpenAI connection established.
      - `{ type: 'event', name: 'AIResponseStart' }`: OpenAI has started generating a response.
      - `{ type: 'event', name: 'AIResponseEnd', finalText: '...' }`: OpenAI has finished the response. `finalText` contains the complete assistant message text.
      - `{ type: 'event', name: 'AISpeechDetected' }`: OpenAI detected the start of user speech (if using server VAD).
      - `{ type: 'event', name: 'AISpeechEnded' }`: OpenAI detected the end of user speech (if using server VAD).
      - `{ type: 'textDelta', text: '...' }`: An incremental part of the assistant's text response. Concatenate these deltas to display the response as it's generated.
      - _(Other potential events or error messages)_

## OpenAI Configuration Details (Hardcoded in Handler)

Note that while `config.js` defines some defaults, the `openaiHandler.js` explicitly requests the following during the `session.update` event:

- **Input Audio Format:** `pcm16` (16kHz, Signed Little Endian - consistent with `config.js`)
- **Output Audio Format:** `pcm16` (16kHz) - This ensures the backend receives raw PCM data.
- **Voice:** `shimmer`
- **Turn Detection:** `server_vad` (Server-Side Voice Activity Detection)
- **Instructions:** `"You are a helpful voice assistant. Respond ONLY in English."`

## Project Structure

```
.
├── .env                 # Local environment variables (API Key, Port) - **DO NOT COMMIT**
├── .gitignore           # Files ignored by git (should include .env, node_modules/, openai_audio_output/)
├── openai_audio_output/ # Directory where generated audio is saved (created automatically)
├── package.json         # Project metadata and dependencies
├── package-lock.json    # Lockfile for reproducible installs
├── server.js            # Main server setup, WebSocket connection handling, CORS
├── openaiHandler.js     # Logic for handling OpenAI API interaction per client
├── config.js            # Loads .env, defines configuration constants (PORT, API Key, OpenAI base URL/model)
└── (Other potential files like tsconfig.json if originally TypeScript)
```

## Troubleshooting

- **Connection Refused:** Ensure the server is running and you are connecting to the correct host and port from your frontend. Check firewall rules.
- **CORS Errors:** Verify the `allowedOrigins` in `server.js` exactly matches your frontend's origin (including protocol `http/https`, host, and port). Check the server console logs for CORS rejection messages.
- **401 Unauthorized (from OpenAI):** Double-check that your `OPENAI_API_KEY` in the `.env` file is correct and valid. Ensure the `.env` file is in the project root where you run the `node` command.
- **Audio Issues:**
  - Verify the frontend is sending audio in the correct format (PCM16, 16kHz, signed, little-endian).
  - Check the server console logs for errors during audio processing or sending/receiving from OpenAI.
  - Examine the `.raw` files saved in `openai_audio_output/` using an audio editor that supports raw PCM import (like Audacity) to verify the received audio.
- **Check Server Logs:** The console output from the backend server provides valuable information about connections, errors, and OpenAI events.

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs, feature requests, or improvements.

<!-- Add contribution guidelines if desired -->

## License

<!-- Specify your license here -->

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details (if you create one).
